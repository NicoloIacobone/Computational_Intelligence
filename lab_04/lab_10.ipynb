{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem : Reinforcement Learning for Tic-Tac-Toe Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to develop a reinforcement learning algorithm, we need to define the following components:\n",
    "* **Environment** \n",
    "  * Possible states in the game environment\n",
    "  * Possible actions in each state\n",
    "  * Rewards for each action in each state\n",
    "\n",
    "* **Agent**\n",
    "  * Policy: the strategy to choose an action given a state\n",
    "  * Value function: the expected return of each state under a given policy\n",
    "  * Model: the agent's representation of the environment\n",
    "\n",
    "* **Learning Algorithm**\n",
    "  * How the agent updates its policy and value function based on the experience\n",
    "\n",
    "In this problem, we will implement a reinforcement learning algorithm for the Tic-Tac-Toe game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State is a namedtuple with two fields, x and o, representing the positions of X and O in the board.\n",
    "\n",
    "MAGIC is a list of values that can be used to check whether a player has won the game. They are based on the magic square of order 3.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "    <td>6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>5</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>3</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In this way, the sum of three numbers in any row, column, or diagonal is always 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from quixo repo\n",
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        '''You can change this for your player if you need to handle state/have memory'''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self):\n",
    "        '''\n",
    "        game: the Quixo game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions and a move among TOP, BOTTOM, LEFT and RIGHT\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.state = State(set(), set()) # actual state of the game\n",
    "        self.trajectory = list() # list of states of the game\n",
    "        self.available_moves = set(range(1, 10)) # available moves\n",
    "        self.winner = None # winner of the game\n",
    "\n",
    "    def play(self, player1, player2):\n",
    "        \"\"\"Play a game between two players\"\"\"\n",
    "        # print(\"available moves: \", self.available_moves)\n",
    "        local_winner = -1\n",
    "        while local_winner == -1 and len(self.available_moves) > 0:\n",
    "            # player1 makes a move\n",
    "            move = player1.make_move(self.state, self.available_moves)\n",
    "            # print(\"player1 move: \", move)\n",
    "\n",
    "            # the move is added to the state\n",
    "            self.state.x.add(move)\n",
    "\n",
    "            # the trajectory is updated\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "\n",
    "            # the move is removed from the available moves\n",
    "            self.available_moves.remove(move)\n",
    "            # print(\"available moves: \", self.available_moves)\n",
    "\n",
    "            # check if the game is over\n",
    "            local_winner = self.check_winner()\n",
    "            if local_winner != -1 or len(self.available_moves) == 0:\n",
    "                break\n",
    "\n",
    "            # same for player2\n",
    "            move = player2.make_move(self.state, self.available_moves)\n",
    "            # print(\"player2 move: \", move)\n",
    "            self.state.o.add(move)\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "            self.available_moves.remove(move)\n",
    "            # print(\"available moves: \", self.available_moves)\n",
    "            local_winner = self.check_winner()\n",
    "            if local_winner != -1 or len(self.available_moves) == 0:\n",
    "                break\n",
    "\n",
    "        self.winner = local_winner\n",
    "        # print(\"winner: \", self.winner)\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"Set the winner: 1 for player1, 2 for player2, -1 for draw\"\"\"\n",
    "        if self.win(self.state.x):\n",
    "            return 1\n",
    "        elif self.win(self.state.o):\n",
    "            return 2\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    # win() function checks if any of the combinations of 3 elements in the set sums to 15 (winning condition)\n",
    "    def win(self, elements):\n",
    "        \"\"\"Checks if elements is winning\"\"\"\n",
    "        return any(sum(c) == 15 for c in combinations(elements, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict is a subclass of dict that returns a default value when the key is not found, so that it is not needed to check whether a key is in the dictionary.\n",
    "\n",
    "frozenset is an immutable version of set, which can be used as a key in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, state, available_moves):\n",
    "        return choice(list(available_moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforcement_player():\n",
    "    def __init__(self, player_index, random_move = 0.0):\n",
    "        self.value_dictionary = defaultdict(float) # state of the game and its value\n",
    "        # self.trajectory = list() # list of states visited during the game, used to update the value_dictionary\n",
    "        self.hit_state = defaultdict(int) # state of the game and how many times it was visited during the training phase\n",
    "        self.epsilon = 0.001 # learning rate\n",
    "        self.player_index = player_index # index of the player (1 or 2)\n",
    "        self.random_move = random_move # a value between 0 and 1, used to choose a random move when training\n",
    "\n",
    "    # in make_moves we have to sometimes choose a random move when training\n",
    "    def make_move(self, state, available_moves):\n",
    "        \"\"\"Returns best move for the actual state\"\"\"\n",
    "        # it checks the value of the new_state for each possible move and returns the move with the highest value\n",
    "        best_move_score = -1\n",
    "        best_move = None\n",
    "        if np.random.rand() < self.random_move:\n",
    "            return choice(list(available_moves))\n",
    "        else:\n",
    "            for move in available_moves:\n",
    "                new_state = deepcopy(state)\n",
    "                hashable_state = (frozenset(new_state.x), frozenset(new_state.o))\n",
    "                if self.player_index == 1:\n",
    "                    new_state.x.add(move)\n",
    "                else:\n",
    "                    new_state.o.add(move)\n",
    "                actual_move_score = self.value_dictionary[hashable_state]\n",
    "                if actual_move_score > best_move_score:\n",
    "                    best_move_score = actual_move_score\n",
    "                    best_move = move\n",
    "\n",
    "        return best_move\n",
    "    \n",
    "    def give_reward(self, reward, trajectory):\n",
    "        \"\"\"Updates the value of the states visited during the game\"\"\"\n",
    "        for state in reversed(trajectory):\n",
    "            hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "            self.hit_state[hashable_state] += 1\n",
    "            self.value_dictionary[hashable_state] += self.epsilon * (reward - self.value_dictionary[hashable_state])\n",
    "\n",
    "    def print_value_dictionary(self):\n",
    "        \"\"\"Prints the value of each state\"\"\"\n",
    "        return sorted(self.value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]\n",
    "    \n",
    "    def set_random_move(self, random_move):\n",
    "        \"\"\"Sets the value of random_move\"\"\"\n",
    "        self.random_move = random_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c66dc37c31f444188d453e543eddeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_game = Game()\n",
    "player1 = reinforcement_player(1, 1)\n",
    "player2 = RandomPlayer()\n",
    "\n",
    "for _ in tqdm(range(500_000)):\n",
    "    # decrease the probability of choosing a random move by 50% every 100_000 games\n",
    "    if _ % 100_000 == 0:\n",
    "        player1.set_random_move(player1.random_move / 10)\n",
    "    test_game = Game()\n",
    "    test_game.play(player1, player2)\n",
    "    if (test_game.winner == 1):\n",
    "        player1.give_reward(1, test_game.trajectory)\n",
    "    elif (test_game.winner == 2):\n",
    "        player1.give_reward(-1, test_game.trajectory)\n",
    "    else:\n",
    "        if(player1.player_index == 1):\n",
    "            player1.give_reward(0.1, test_game.trajectory)\n",
    "        else:\n",
    "            player1.give_reward(0.3, test_game.trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3060\n",
      "3059\n",
      "[((frozenset({1, 2, 3, 4, 8}), frozenset({9, 5, 6, 7})), 0.9999999999999303), ((frozenset({1, 2, 3, 4, 9}), frozenset({8, 5, 6, 7})), 0.9999999999999292), ((frozenset({1, 2, 3, 4}), frozenset({8, 5, 6, 7})), 0.9999999999998813), ((frozenset({1, 2, 3, 4}), frozenset({9, 5, 6, 7})), 0.9999999999998788), ((frozenset({1, 2, 3, 5, 7}), frozenset({8, 9, 4, 6})), 0.9999999998844641), ((frozenset({1, 2, 3, 5, 8}), frozenset({9, 4, 6, 7})), 0.9999999998780503), ((frozenset({1, 2, 3, 5, 9}), frozenset({8, 4, 6, 7})), 0.9999999998770702), ((frozenset({1, 2, 3, 5}), frozenset({8, 9, 4, 6})), 0.9999999998176714), ((frozenset({1, 2, 3, 5}), frozenset({8, 4, 6, 7})), 0.9999999998094659), ((frozenset({1, 2, 3, 5}), frozenset({9, 4, 6, 7})), 0.9999999998071645)]\n"
     ]
    }
   ],
   "source": [
    "print(len(player1.value_dictionary))\n",
    "print(len(player1.hit_state))\n",
    "print(player1.print_value_dictionary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94f1068a6594a6f972f2644baf99e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins:  52.25 %\n",
      "draws:  30.93 %\n",
      "losses:  16.82 %\n"
     ]
    }
   ],
   "source": [
    "player1.set_random_move(0.0)\n",
    "\n",
    "win_rate = 0\n",
    "draw_rate = 0\n",
    "loss_rate = 0\n",
    "for _ in tqdm(range(10000)):\n",
    "    test_game = Game()\n",
    "    test_game.play(player1, player2)\n",
    "    if (test_game.winner == 1):\n",
    "        win_rate += 1\n",
    "    elif (test_game.winner == -1):\n",
    "        draw_rate += 1\n",
    "    else:\n",
    "        loss_rate += 1\n",
    "\n",
    "print(\"wins: \", win_rate/100, \"%\")\n",
    "print(\"draws: \", draw_rate/100, \"%\")\n",
    "print(\"losses: \", loss_rate/100, \"%\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computational_Intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
