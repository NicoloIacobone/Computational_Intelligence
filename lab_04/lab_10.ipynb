{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem : Reinforcement Learning for Tic-Tac-Toe Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to develop a reinforcement learning algorithm, we need to define the following components:\n",
    "* **Environment** \n",
    "  * Possible states in the game environment\n",
    "  * Possible actions in each state\n",
    "  * Rewards for each action in each state\n",
    "\n",
    "* **Agent**\n",
    "  * Policy: the strategy to choose an action given a state\n",
    "  * Value function: the expected return of each state under a given policy\n",
    "  * Model: the agent's representation of the environment\n",
    "\n",
    "* **Learning Algorithm**\n",
    "  * How the agent updates its policy and value function based on the experience\n",
    "\n",
    "In this problem, we will implement a reinforcement learning algorithm for the Tic-Tac-Toe game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State is a namedtuple with two fields, x and o, representing the positions of X and O in the board.\n",
    "\n",
    "MAGIC is a list of values that can be used to check whether a player has won the game. They are based on the magic square of order 3.\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>7</td>\n",
    "    <td>6</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>5</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>3</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "In this way, the sum of three numbers in any row, column, or diagonal is always 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stolen from quixo repo\n",
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        '''You can change this for your player if you need to handle state/have memory'''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_move(self):\n",
    "        '''\n",
    "        game: the Quixo game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions and a move among TOP, BOTTOM, LEFT and RIGHT\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.state = State(set(), set()) # actual state of the game\n",
    "        self.trajectory = list() # list of states of the game\n",
    "        self.available_moves = set(range(1, 10)) # available moves\n",
    "        self.winner = None # winner of the game\n",
    "\n",
    "    def play(self, player1, player2):\n",
    "        \"\"\"Play a game between two players\"\"\"\n",
    "        local_winner = -1\n",
    "        while local_winner == -1 and len(self.available_moves) > 0:\n",
    "            if isinstance(player1, Human_Player) or isinstance(player2, Human_Player):\n",
    "                self.print_board()\n",
    "            # player1 makes a move\n",
    "            move = player1.make_move(self.state, self.available_moves)\n",
    "\n",
    "            # the move is added to the state\n",
    "            self.state.x.add(move)\n",
    "\n",
    "            # the trajectory is updated\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "\n",
    "            # the move is removed from the available moves\n",
    "            self.available_moves.remove(move)\n",
    "\n",
    "            # check if the game is over\n",
    "            local_winner = self.check_winner()\n",
    "            if local_winner != -1 or len(self.available_moves) == 0:\n",
    "                break\n",
    "\n",
    "            if isinstance(player1, Human_Player) or isinstance(player2, Human_Player):\n",
    "                self.print_board()\n",
    "            # same for player2\n",
    "            move = player2.make_move(self.state, self.available_moves)\n",
    "            self.state.o.add(move)\n",
    "            self.trajectory.append(deepcopy(self.state))\n",
    "            self.available_moves.remove(move)\n",
    "            local_winner = self.check_winner()\n",
    "            if local_winner != -1 or len(self.available_moves) == 0:\n",
    "                break\n",
    "\n",
    "        self.winner = local_winner\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"Set the winner: 1 for player1, 2 for player2, -1 for draw\"\"\"\n",
    "        if self.win(self.state.x):\n",
    "            return 1\n",
    "        elif self.win(self.state.o):\n",
    "            return 2\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    # win() function checks if any of the combinations of 3 elements in the set sums to 15 (winning condition)\n",
    "    def win(self, elements):\n",
    "        \"\"\"Checks if elements is winning\"\"\"\n",
    "        return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "    \n",
    "    def print_board(self):\n",
    "        \"\"\"Nicely prints the board\"\"\"\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                i = r * 3 + c\n",
    "                if MAGIC[i] in self.state.x:\n",
    "                    print('❌', end='')\n",
    "                elif MAGIC[i] in self.state.o:\n",
    "                    print('⭕️', end='')\n",
    "                else:\n",
    "                    print('⬜️', end='')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defaultdict is a subclass of dict that returns a default value when the key is not found, so that it is not needed to check whether a key is in the dictionary.\n",
    "\n",
    "frozenset is an immutable version of set, which can be used as a key in a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # just choose randomly among the available moves\n",
    "    def make_move(self, state, available_moves):\n",
    "        return choice(list(available_moves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reinforcement_player(Player):\n",
    "    def __init__(self, player_index, random_move = 0.0):\n",
    "        self.value_dictionary = defaultdict(float) # state of the game and its value\n",
    "        self.hit_state = defaultdict(int) # state of the game and how many times it was visited during the training phase\n",
    "        self.epsilon = 0.1 # learning rate\n",
    "        self.player_index = player_index # index of the player (1 or 2)\n",
    "        self.random_move = random_move # a value between 0 and 1, used to choose a random move when training\n",
    "\n",
    "    # in make_moves we have to sometimes choose a random move when training\n",
    "    def make_move(self, state, available_moves):\n",
    "        \"\"\"Returns best move for the actual state\"\"\"\n",
    "        # it checks the value of the new_state for each possible move and returns the move with the highest value\n",
    "        best_move_score = -10_000\n",
    "        best_move = None\n",
    "        if np.random.rand() < self.random_move:\n",
    "            return choice(list(available_moves))\n",
    "        else:\n",
    "            for move in available_moves:\n",
    "                new_state = deepcopy(state)\n",
    "                if self.player_index == 1:\n",
    "                    new_state.x.add(move)\n",
    "                elif self.player_index == 2:\n",
    "                    new_state.o.add(move)\n",
    "                else:\n",
    "                    raise ValueError(\"player_index must be 1 or 2\")\n",
    "                hashable_state = (frozenset(new_state.x), frozenset(new_state.o))\n",
    "                actual_move_score = self.value_dictionary[hashable_state]\n",
    "                if actual_move_score > best_move_score:\n",
    "                    best_move_score = actual_move_score\n",
    "                    best_move = move\n",
    "\n",
    "        return best_move\n",
    "    \n",
    "    def give_reward(self, reward, trajectory):\n",
    "        \"\"\"Updates the value of the states visited during the game\"\"\"\n",
    "        for state in reversed(trajectory):\n",
    "            hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "            self.hit_state[hashable_state] += 1\n",
    "            self.value_dictionary[hashable_state] += self.epsilon * (reward - self.value_dictionary[hashable_state])\n",
    "            reward = self.value_dictionary[hashable_state]\n",
    "\n",
    "    def print_value_dictionary(self):\n",
    "        \"\"\"Prints the value of each state\"\"\"\n",
    "        return sorted(self.value_dictionary.items(), key=lambda e: e[1], reverse=True)\n",
    "    \n",
    "    # used to switch between train and test phases\n",
    "    def set_random_move(self, random_move):\n",
    "        \"\"\"Sets the value of random_move\"\"\"\n",
    "        self.random_move = random_move\n",
    "\n",
    "    # used to switch between player 1 and player 2\n",
    "    def set_player_index(self, player_index):\n",
    "        \"\"\"Sets the value of player_index\"\"\"\n",
    "        self.player_index = player_index\n",
    "\n",
    "    # creates the policy file where it is stored the value of each state\n",
    "    def create_policy(self):\n",
    "        \"\"\"Creates the policy file\"\"\"\n",
    "        fw = open('policy_' + str(self.player_index), 'wb')\n",
    "        pickle.dump(self.value_dictionary, fw)\n",
    "        fw.close()\n",
    "\n",
    "    # loads the policy file\n",
    "    def load_policy(self):\n",
    "        \"\"\"Loads the policy file\"\"\"\n",
    "        fr = open('policy_' + str(self.player_index), 'rb')\n",
    "        self.value_dictionary = pickle.load(fr)\n",
    "        fr.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_Player(Player):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # just ask the user to choose a move between the available ones\n",
    "    def make_move(self, state, available_moves):\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')  # Clear the terminal\n",
    "        print(\"Available moves: \", available_moves)\n",
    "        move = input(\"Your move: \")\n",
    "        return int(move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Player 1 vs Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e99bd3969b743f1afe28143f9d7310c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player1 = reinforcement_player(1, 0.3)\n",
    "random_player = RandomPlayer()\n",
    "\n",
    "# training phase\n",
    "for _ in tqdm(range(100_000)):\n",
    "    game = Game()\n",
    "    game.play(player1, random_player)\n",
    "    if game.winner == 1:\n",
    "        player1.give_reward(1, game.trajectory)\n",
    "    elif game.winner == 2:\n",
    "        player1.give_reward(-1, game.trajectory)\n",
    "    else:\n",
    "        player1.give_reward(0, game.trajectory)\n",
    "\n",
    "# save the policy\n",
    "player1.create_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Player 1 vs Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa296c1d20b4276827c73ac87fad66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 wins: 98.9 %\n",
      "Player 1 losses: 0.0 %\n",
      "Player 1 draws: 1.1 %\n"
     ]
    }
   ],
   "source": [
    "player1 = reinforcement_player(1, 0.0)\n",
    "player1.load_policy()\n",
    "wins_player1 = 0\n",
    "loss_player1 = 0\n",
    "draw_player1 = 0\n",
    "\n",
    "# testing phase\n",
    "for _ in tqdm(range(10_000)):\n",
    "    game = Game()\n",
    "    game.play(player1, random_player)\n",
    "    if game.winner == 1:\n",
    "        wins_player1 += 1\n",
    "    elif game.winner == 2:\n",
    "        loss_player1 += 1\n",
    "    else:\n",
    "        draw_player1 += 1\n",
    "\n",
    "print(f\"Player 1 wins: {wins_player1/100} %\")\n",
    "print(f\"Player 1 losses: {loss_player1/100} %\")\n",
    "print(f\"Player 1 draws: {draw_player1/100} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Player vs Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89816f734f3e4d789b917d771224951f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player2 = reinforcement_player(2, 0.3)\n",
    "random_player = RandomPlayer()\n",
    "\n",
    "# training phase\n",
    "for _ in tqdm(range(100_000)):\n",
    "    game = Game()\n",
    "    game.play(random_player, player2)\n",
    "    if game.winner == 1:\n",
    "        player2.give_reward(-1, game.trajectory)\n",
    "    elif game.winner == 2:\n",
    "        player2.give_reward(1, game.trajectory)\n",
    "    else:\n",
    "        player2.give_reward(0, game.trajectory)\n",
    "\n",
    "# save the policy\n",
    "player2.create_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Random Player vs Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4706f9f705d743f8a2ee6166a5e480fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 2 wins: 91.16 %\n",
      "Player 2 losses: 1.06 %\n",
      "Player 2 draws: 7.78 %\n"
     ]
    }
   ],
   "source": [
    "player2 = reinforcement_player(2, 0.0)\n",
    "player2.load_policy()\n",
    "wins_player2 = 0\n",
    "loss_player2 = 0\n",
    "draw_player2 = 0\n",
    "debug_val = 0\n",
    "\n",
    "# testing phase\n",
    "for _ in tqdm(range(10_000)):\n",
    "    game = Game()\n",
    "    game.play(random_player, player2)\n",
    "    if game.winner == 1:\n",
    "        loss_player2 += 1\n",
    "    elif game.winner == 2:\n",
    "        wins_player2 += 1\n",
    "    else:\n",
    "        draw_player2 += 1\n",
    "\n",
    "print(f\"Player 2 wins: {wins_player2/100} %\")\n",
    "print(f\"Player 2 losses: {loss_player2/100} %\")\n",
    "print(f\"Player 2 draws: {draw_player2/100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Player 1 vs Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1712d1a1d14950a516da00492c94aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "player1 = reinforcement_player(1, 0.3)\n",
    "player2 = reinforcement_player(2, 0.3)\n",
    "\n",
    "# training phase\n",
    "for _ in tqdm(range(100_000)):\n",
    "    game = Game()\n",
    "    game.play(player1, player2)\n",
    "    if game.winner == 1:\n",
    "        player1.give_reward(1, game.trajectory)\n",
    "        player2.give_reward(-1, game.trajectory)\n",
    "    elif game.winner == 2:\n",
    "        player1.give_reward(-1, game.trajectory)\n",
    "        player2.give_reward(1, game.trajectory)\n",
    "    else:\n",
    "        player1.give_reward(0, game.trajectory)\n",
    "        player2.give_reward(0, game.trajectory)\n",
    "\n",
    "# save the policies\n",
    "player1.create_policy()\n",
    "player2.create_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Player 1 vs Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb026982f914a7abe1adc9eda284c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 wins: 0.0 %\n",
      "Player 1 losses: 0.0 %\n",
      "Player 1 draws: 100.0 %\n",
      "--------------------------------------\n",
      "Player 2 wins: 0.0 %\n",
      "Player 2 losses: 0.0 %\n",
      "Player 2 draws: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "player1 = reinforcement_player(1, 0.0)\n",
    "player1.load_policy()\n",
    "player2 = reinforcement_player(2, 0.0)\n",
    "player2.load_policy()\n",
    "\n",
    "wins_player1 = 0\n",
    "loss_player1 = 0\n",
    "draw_player1 = 0\n",
    "\n",
    "wins_player2 = 0\n",
    "loss_player2 = 0\n",
    "draw_player2 = 0\n",
    "\n",
    "# testing phase\n",
    "for _ in tqdm(range(10_000)):\n",
    "    game = Game()\n",
    "    game.play(player1, player2)\n",
    "    if game.winner == 1:\n",
    "        wins_player1 += 1\n",
    "        loss_player2 += 1\n",
    "    elif game.winner == 2:\n",
    "        loss_player1 += 1\n",
    "        wins_player2 += 1\n",
    "    else:\n",
    "        draw_player1 += 1\n",
    "        draw_player2 += 1\n",
    "\n",
    "print(f\"Player 1 wins: {wins_player1/100} %\")\n",
    "print(f\"Player 1 losses: {loss_player1/100} %\")\n",
    "print(f\"Player 1 draws: {draw_player1/100} %\")\n",
    "print(\"--------------------------------------\")\n",
    "print(f\"Player 2 wins: {wins_player2/100} %\")\n",
    "print(f\"Player 2 losses: {loss_player2/100} %\")\n",
    "print(f\"Player 2 draws: {draw_player2/100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMAGIC = [2, 7, 6, \\n         9, 5, 1, \\n         4, 3, 8]\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAGIC = [2, 7, 6, \n",
    "         9, 5, 1, \n",
    "         4, 3, 8]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Human vs Player 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️\n",
      "⬜️⬜️⬜️\n",
      "\n",
      "\u001b[H\u001b[2JAvailable moves:  {1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬜️⬜️⬜️\n",
      "⬜️❌⬜️\n",
      "⬜️⬜️⬜️\n",
      "\n",
      "⬜️⬜️⬜️\n",
      "⬜️❌⬜️\n",
      "⭕️⬜️⬜️\n",
      "\n",
      "\u001b[H\u001b[2JAvailable moves:  {1, 2, 3, 6, 7, 8, 9}\n",
      "❌⬜️⬜️\n",
      "⬜️❌⬜️\n",
      "⭕️⬜️⬜️\n",
      "\n",
      "❌⬜️⬜️\n",
      "⬜️❌⬜️\n",
      "⭕️⬜️⭕️\n",
      "\n",
      "\u001b[H\u001b[2JAvailable moves:  {1, 3, 6, 7, 9}\n",
      "❌⬜️⬜️\n",
      "⬜️❌⬜️\n",
      "⭕️❌⭕️\n",
      "\n",
      "❌⭕️⬜️\n",
      "⬜️❌⬜️\n",
      "⭕️❌⭕️\n",
      "\n",
      "\u001b[H\u001b[2JAvailable moves:  {1, 6, 9}\n",
      "❌⭕️⬜️\n",
      "❌❌⬜️\n",
      "⭕️❌⭕️\n",
      "\n",
      "❌⭕️⬜️\n",
      "❌❌⭕️\n",
      "⭕️❌⭕️\n",
      "\n",
      "\u001b[H\u001b[2JAvailable moves:  {6}\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "human_player = Human_Player()\n",
    "game = Game()\n",
    "game.play(human_player, player2)\n",
    "print(game.winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computational_Intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
